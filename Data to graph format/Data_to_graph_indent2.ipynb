{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DbRdnWSd3hxb"
   },
   "outputs": [],
   "source": [
    "#  pip install --user git+https://github.com/LAL/trackml-library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iG5KxZDA4GIe"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "# def format_pytorch_version(version):\n",
    "#   return version.split('+')[0]\n",
    "\n",
    "# TORCH_version = torch.__version__\n",
    "# TORCH = format_pytorch_version(TORCH_version)\n",
    "\n",
    "# def format_cuda_version(version):\n",
    "#   return 'cu' + version.replace('.', '')\n",
    "\n",
    "# CUDA_version = torch.version.cuda\n",
    "# CUDA = format_cuda_version(CUDA_version)\n",
    "\n",
    "# !pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "# !pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "# !pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "# !pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "# !pip install torch-geometric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQ6oxDZI4OH9"
   },
   "outputs": [],
   "source": [
    "#basic libraries\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import trackml.dataset\n",
    "import sys    \n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "#proccesing data \n",
    "from data_preprocess import *    # reads data from data_preprocess.py\n",
    "#ML libraries\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import namedtuple\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bvNwmHcm7xrH"
   },
   "source": [
    "## Data Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "98vs5Wpm5SpI"
   },
   "outputs": [],
   "source": [
    "#######################ENRICH DATA#####################################\n",
    "# Calculate derived hits variables\n",
    "def initial_params(df):\n",
    "  \"\"\" Calculate momentum radius, phi and ita\"\"\"\n",
    "  df['pt'] = np.sqrt(df.tpx**2 + df.tpy**2)\n",
    "  df['r'] = np.sqrt(df.x**2 + df.y**2)\n",
    "  df['phi'] = np.arctan2(df.y, df.x)\n",
    "  def calc_ita(r, z):\n",
    "      theta = np.arctan2(r, z)\n",
    "      return -1. * np.log(np.tan(theta / 2.)), theta\n",
    "  df['ita'], df['theta'] = calc_ita(df['r'],df['z'])\n",
    "\n",
    "  return df\n",
    "\n",
    "def ordered_layer_id(df):\n",
    "    \"\"\"order layers from 0 to N\"\"\"\n",
    "    df = df.sort_values(by=['volume_id','layer_id','z']).reset_index(drop = True) \n",
    "    new_layers_array = np.zeros(len(df))\n",
    "    i = 0\n",
    "    idx = 0\n",
    "    for vol in df.volume_id.unique():\n",
    "      temp_df = df.loc[df.volume_id == vol]\n",
    "      for layer in temp_df.layer_id.unique():\n",
    "        temp2_df = temp_df.loc[temp_df.layer_id == layer]\n",
    "        new_layers_array[idx:len(temp2_df)+idx]=i\n",
    "        i+=1\n",
    "        idx+=len(temp2_df)\n",
    "    df['new_layer_id'] = new_layers_array.astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V3O8x87u5Ewe"
   },
   "outputs": [],
   "source": [
    "#######################SECTION FILTERS#####################################\n",
    "# firts use the geomerty of the detector to reduce data, barrel, layers, and sectioning\n",
    "def sections(df,phi_sec,ita_sec, overlap_phi = 0, overlap_ita = 0):\n",
    "   \n",
    "  #use only important columns\n",
    "  df = df[['new_hit_id', 'new_particle_id', 'particle_id','weight','new_layer_id','layer_id','volume_id','x', 'y', 'z','pt','tpx','tpy', 'r', 'phi','ita','event_id']]\n",
    "  phi_range = (-np.pi, np.pi)\n",
    "  ita_range =  (-3, 3)\n",
    "  n_phi_sections = phi_sec          # Towards a realistic track reconstruction algorithm based\n",
    "  n_ita_sections = ita_sec          #on graph neural networks for the HL-LHC\n",
    "\n",
    "  phi_edges = np.linspace(*phi_range, num=n_phi_sections+1)   #4 sections in range of phi \n",
    "  ita_edges = np.linspace(*ita_range, num=n_ita_sections+1)   #2 sections in range of ita\n",
    "  def split_detector_sections(df, phi_edges, ita_edges):\n",
    "      \"\"\"Split hits according to provided phi and ita boundaries.\"\"\"\n",
    "      hits_sections = []\n",
    "      # Loop over sections on phi\n",
    "      for i in range(len(phi_edges) - 1):\n",
    "          if np.round(abs(phi_edges[i] - overlap_phi),4)>= 3.1416:     # when angle - overlap is smaller than -pi make angle positive and join dfs in range \n",
    "            \n",
    "            phi_max = phi_edges[i+1] + overlap_phi\n",
    "            phi_min = ((phi_edges[i] - overlap_phi) + 2*np.pi) \n",
    "            pos = df.loc[df.phi > phi_min]\n",
    "            neg = df.loc[df.phi <= phi_max]\n",
    "            phi_hits = pd.concat((pos,neg))\n",
    "            \n",
    "            \n",
    "          elif np.round(abs(phi_edges[i+1] + overlap_phi),4)>= 3.1416:  # when angle + overlap is bigger than pi make angle negative and join dfs in range \n",
    "            \n",
    "            phi_min = phi_edges[i] - overlap_phi\n",
    "            phi_max = ((phi_edges[i+1] + overlap_phi) - 2*np.pi) \n",
    "            pos = df.loc[df.phi > phi_min]\n",
    "            neg = df.loc[df.phi <= phi_max]\n",
    "            phi_hits = pd.concat((pos,neg))\n",
    "\n",
    "          else:\n",
    "            \n",
    "            phi_min = phi_edges[i] - overlap_phi\n",
    "            phi_max = phi_edges[i+1] + overlap_phi\n",
    "            phi_hits = df[(df.phi >= phi_min) & (df.phi < phi_max)]\n",
    "        \n",
    "\n",
    "          \n",
    "        \n",
    "          # Center these hits on phi=0\n",
    "          centered_phi = phi_hits.phi - (phi_min + phi_max) / 2\n",
    "          phi_hits = phi_hits.assign(sec_phi=centered_phi, phi_section=i)\n",
    "          #loop over sections on phi\n",
    "          for j in range(len(ita_edges) - 1):\n",
    "              ita_min, ita_max = ita_edges[j] - overlap_ita, ita_edges[j+1] + overlap_phi\n",
    "              # Select hits in this ita section\n",
    "              sec_hits = phi_hits[(df['ita'] >= ita_min) & (df['ita'] < ita_max)]\n",
    "              hits_sections.append(sec_hits.assign(ita_section=j))\n",
    "       #create new hits ids \n",
    "      for i ,dfs in enumerate(hits_sections):\n",
    "        dfs = dfs.reset_index(drop = True)\n",
    "        dfs.new_hit_id = dfs.index\n",
    "        hits_sections[i] = dfs\n",
    "\n",
    "      return hits_sections\n",
    "  return  split_detector_sections(df, phi_edges, ita_edges)\n",
    "\n",
    "\n",
    "#barrel\n",
    "def volume_filter(df, volume_list):\n",
    "  \"\"\" \n",
    "     Returns only hits in volumes given in volume_list \n",
    "  \"\"\"\n",
    "  return df.loc[df.volume_id.isin(volume_list)]\n",
    "\n",
    "#choosen layers\n",
    "def layer_filter(df, layer_list):\n",
    "  \"\"\" \n",
    "     Returns only hits in volumes given in volume_list \n",
    "  \"\"\"\n",
    "  return df.loc[df.new_layer_id.isin(layer_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "exjfaEUu4yxm"
   },
   "outputs": [],
   "source": [
    "\n",
    "#######################INITIAL FILTERS#####################################\n",
    "def remove_noise(df):\n",
    "  return df.loc[df.particle_id!=0]\n",
    "def weight_filter(df):\n",
    "  \"\"\"\n",
    "     Remove weights = 0\n",
    "  \"\"\"\n",
    "  return df.loc[df.weight!=0]\n",
    "\n",
    "\n",
    "\n",
    "def db_hit_layer_filter(df):\n",
    "  \"\"\"\n",
    "     Remove double hits in the same layer\n",
    "  \"\"\"\n",
    "  return df.loc[df.duplicated(subset=['new_particle_id', 'layer_id','volume_id'])==False]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xi38nVLU57gy"
   },
   "outputs": [],
   "source": [
    "#######################CALCULATION FILTERS#####################################\n",
    "def p_t_filter(df, min_pt):\n",
    "  \"\"\"Removes particles < min_pt\"\"\"\n",
    "  pt_1 = df.loc[df['pt']>min_pt].new_particle_id.unique()\n",
    "  return df.loc[df['new_particle_id'].isin(pt_1)]\n",
    "def ita_filter(df, min_ita, max_ita):\n",
    "  \"\"\"keeps particles in the range (min_ita,max_ita)\"\"\"\n",
    "  ita_f = df.loc[(df['ita']<min_ita) | (df['ita']>max_ita)].new_particle_id.unique()\n",
    "  return df.loc[~df['new_particle_id'].isin(ita_f)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4hVp-5y75cy"
   },
   "source": [
    "## Node Edge Conections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5zewC9_v6VYS"
   },
   "outputs": [],
   "source": [
    "#######################MORE THAN 4 FILTERS#####################################\n",
    "def nhit_filter(df, nhits):\n",
    "  \"\"\"\n",
    "     Remove less than n hits\n",
    "  \"\"\"\n",
    "  #find the amounts of hits per particle\n",
    "  new_nhits = df.new_particle_id.value_counts().rename_axis('new_particle_id').reset_index(name='new_nhits') \n",
    "  #merge the new hhits on new particle id \n",
    "  df = df.merge(new_nhits, on='new_particle_id')\n",
    "  return df.loc[df.new_nhits >= nhits]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FnVT3iRo8Mbx"
   },
   "outputs": [],
   "source": [
    "#######################CREATE EDGES#####################################\n",
    "def calc_dphi(phi1, phi2):\n",
    "  \"\"\"Computes phi2-phi1 \"\"\"\n",
    "  dphi = phi2 - phi1\n",
    "   \n",
    "  return dphi\n",
    "\n",
    "  \n",
    "def get_param_distr(df):\n",
    "  \"\"\" create edges and calculates dphi, dz,dr,dita,dR,phi_slope, and z0\"\"\"\n",
    "\n",
    "  dR_list, phi_slope_list,z0_list, hits_pair_list = [], [], [],[]\n",
    "  #################CREATE EDGES#####################################\n",
    "  # some particle ids dont have hist in certain layers therefore is not possible to just loop through conecutive layers, \n",
    "  # it is neccesary to loop through each p_id to see the set of consecutive layers and conect those layers\n",
    "  \n",
    "  for p_id in df.new_particle_id.unique():\n",
    "    \n",
    "    temp_df = df.loc[df.new_particle_id==p_id]\n",
    "    cons_layers = temp_df.new_layer_id.unique()\n",
    "    # loop trhough layers to generate hits pair(section of track) on consecutive layers \n",
    "    for i in range(len(cons_layers)-1):\n",
    "        \n",
    "      hits1 = temp_df.loc[temp_df.new_layer_id == cons_layers[i]]\n",
    "      hits2 = df.loc[df.new_layer_id == cons_layers[i+1]]\n",
    "      keys = ['event_id', 'new_hit_id','new_layer_id','new_particle_id','r', 'phi', 'z','ita','x','y']\n",
    "      hit_pairs  = hits1[keys].reset_index(drop = True).merge(hits2[keys].reset_index(drop = True), on='event_id', suffixes=('_1', '_2'))\n",
    "      \n",
    "      hit_pairs['y'] = hit_pairs.new_particle_id_1 == hit_pairs.new_particle_id_2\n",
    "\n",
    "\n",
    "      hits_pair_list.append(hit_pairs)\n",
    "    \n",
    "    # concatenate firts 2 hit pairs \n",
    "  df_concat = pd.concat([hits_pair_list[0], hits_pair_list[1]], ignore_index=True)\n",
    "  # concatenate firts the rest hit pairs\n",
    "  for i in range(len(hits_pair_list) -2):\n",
    "    df_concat = pd.concat([df_concat, hits_pair_list[i+2]], ignore_index=True)\n",
    "\n",
    "  #############CALCULATE PARAMTERS#####################################  \n",
    "  df_concat['dphi'] = calc_dphi(df_concat.phi_1, df_concat.phi_2)\n",
    "  df_concat['dz'] = df_concat.z_2 - df_concat.z_1\n",
    "  df_concat['dr'] = df_concat.r_2 - df_concat.r_1\n",
    "  df_concat['dita'] = df_concat.ita_2 - df_concat.ita_1\n",
    "  df_concat['dR'] = np.sqrt(df_concat['dita']**2 + df_concat['dphi']**2)\n",
    "  df_concat['phi_slope'] = df_concat['dphi'] / df_concat['dr']\n",
    "  df_concat['z0'] = df_concat.z_1 - df_concat.r_1 * df_concat['dz'] / df_concat['dr']\n",
    "  return df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uPtLxWtE9L_Y"
   },
   "outputs": [],
   "source": [
    "def edge_filter(df):\n",
    "  \"\"\"reduce amount of edges using calculated parameters\"\"\"\n",
    "  min_z0 =  -320\n",
    "  max_z0 = 520\n",
    "  df = df.loc[(df.z0>=min_z0) & (df.z0<=max_z0)]\n",
    "  \n",
    "  \n",
    "  max_dr = 0.5\n",
    "  df = df.loc[df.dR<=max_dr]\n",
    "  \n",
    "  min_phi_slope =  -.0055\n",
    "  max_phi_slope = .0055\n",
    "  df =df.loc[(df.phi_slope>=min_phi_slope) & (df.phi_slope<max_phi_slope)] \n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1F-GLjv49inb"
   },
   "source": [
    "## Inbalanced Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0hQIhKCV9wDP"
   },
   "outputs": [],
   "source": [
    "#############BALANCE DATASET##################################### \n",
    "def resample_data(df):\n",
    "  \"\"\"Returns balanced dataset\"\"\"\n",
    "  X = df.drop(['y'], axis=1)\n",
    "  y = df.y\n",
    "      \n",
    "  #undersample data \n",
    "  rus = RandomUnderSampler(random_state=42)\n",
    "  X_res, y_res = rus.fit_resample(X, y)\n",
    " \n",
    "  df = X_res\n",
    "  df['y'] = y_res\n",
    "  return df.sort_values(by=['new_layer_id_1','z_1'])\n",
    "\n",
    "##########RE-LABLE HIT ID#####################################\n",
    "def New_hit_id(df):\n",
    "  \"\"\"returns dataset with ordered hit id\"\"\"\n",
    "  df = df.sort_values(by=['new_layer_id_1','z_1'])\n",
    "  df_hits = pd.DataFrame()\n",
    "  df_hits['unique_hits'] =  pd.concat([df['new_hit_id_1'], df['new_hit_id_2']]).unique()\n",
    "  df_hits = df_hits.sort_values('unique_hits').reset_index(drop = True)\n",
    "  df_hits['new_hit'] = df_hits.index\n",
    "\n",
    "  newest_id1 = np.zeros(len(df))\n",
    "  \n",
    "  for i, id in enumerate(df['new_hit_id_1']):\n",
    "    newest_id1[i] = df_hits.new_hit.loc[df_hits.unique_hits==id]\n",
    "  df['hit_id_1'] = newest_id1.astype(int)\n",
    "  newest_id2 = np.zeros(len(df))\n",
    "  \n",
    "  for i, id in enumerate(df['new_hit_id_2']):\n",
    "    newest_id2[i] = df_hits.new_hit.loc[df_hits.unique_hits==id]\n",
    "  df['hit_id_2'] = newest_id2.astype(int)\n",
    "  df = df.sort_values('hit_id_1')\n",
    "\n",
    "  \n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wFwVjzfP-8Z8"
   },
   "source": [
    "## Data in Graph Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d1_jhXSQ_GzZ"
   },
   "outputs": [],
   "source": [
    "def data_to_graph(df,directed=True):\n",
    "  \"\"\"Returns graph data into pytorch format\"\"\"\n",
    "  edge_attr = df[['dphi', 'dz', 'dr', 'dita', 'dR','phi_slope', 'z0']]\n",
    "  edge_index = df[['hit_id_1','hit_id_2']]\n",
    "  y = df.y\n",
    "  df1 = pd.DataFrame()\n",
    "  df2 = pd.DataFrame()\n",
    "  df1[['hit_id','r', 'phi', 'z', 'ita', 'x','y' ]] = df[['hit_id_1','r_1', 'phi_1', 'z_1', 'ita_1','x_1','y_1' ]]\n",
    "  df2[['hit_id','r', 'phi', 'z', 'ita', 'x','y' ]] = df[['hit_id_2','r_2', 'phi_2', 'z_2', 'ita_2','x_2','y_2' ]]\n",
    "  x = pd.concat([df1,df2])\n",
    "  x = x.drop_duplicates(subset=['hit_id'])\n",
    "  x = x.sort_values('hit_id')\n",
    "  x = x[['r', 'phi', 'z', 'ita', 'x','y']]\n",
    "\n",
    "  if directed:\n",
    "    edge_attr = torch.from_numpy(edge_attr.to_numpy()).type(torch.FloatTensor)\n",
    "    edge_index = torch.from_numpy(edge_index.to_numpy())   # this is double \n",
    "    y = torch.from_numpy(y.to_numpy()).type(torch.FloatTensor)\n",
    "    x = torch.from_numpy(x.to_numpy()).type(torch.FloatTensor)    #change to float  x[['r', 'phi', 'z', 'ita', 'x','y']]\n",
    "    data = Data(x=x, edge_index=edge_index.T,edge_attr=edge_attr,y=y)\n",
    "  else:\n",
    "\n",
    "    edge_index_und = torch.stack([torch.cat([edge_index[:,0], edge_index[:,1]], dim=0), torch.cat([edge_index[:,1], edge_index[:,0]], dim=0)], dim = 0)\n",
    "    edge_attr_und = torch.cat([edge_attr, -1*edge_attr], dim=0)\n",
    "    y_und = torch.cat([y,y])\n",
    "    data = Data(x=x,edge_index= edge_index_und,edge_attr = edge_attr_und,y = y_und)#, first_layer_hits = first_layer_hits  )\n",
    "  return data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z61QVeG-_hQ1"
   },
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eSDiMOMK_mXz"
   },
   "outputs": [],
   "source": [
    "##########CONSTANTS#####################################\n",
    "volume_list = [8,13,17]   #barrel\n",
    "pt_cut = 1\n",
    "ita_min = -3\n",
    "ita_max = 3\n",
    "nhits = 5\n",
    "layer_list = [0,1,2,3,4,5,7,8]\n",
    "\n",
    "\n",
    "def run_and_save(event_start,event_end,path_file, SAVE=True,BALANCE=False):\n",
    "  \"\"\"transforms raw data into graph format as outlined in the report and save it\"\"\"\n",
    "  for i in range(event_start,event_end):\n",
    "    file_idx =0\n",
    "    #read data \n",
    "    hits, particles, truth = read_all_data(1000 + i,1000 + (i+1))  \n",
    "    df = merge_dfs(hits, particles, truth)\n",
    "    \n",
    "    #firts volume filter\n",
    "    df = volume_filter(df, volume_list)\n",
    "  \n",
    "    #fits stage create new_layer_id \n",
    "    df = ordered_layer_id(df)\n",
    "    \n",
    "    #then return only layers in list \n",
    "    df = layer_filter(df, layer_list)\n",
    "\n",
    "    #calculate the extra parameters\n",
    "    df = initial_params(df)\n",
    "\n",
    "    #section detector\n",
    "    df_sections = sections(df,4,2, overlap_phi = 0.26, overlap_ita = 0.5)\n",
    "\n",
    "     #loop throug all sections \n",
    "  for sec,dfs in enumerate(df_sections):\n",
    "    #initial filters\n",
    "    dfs = dfs.pipe(remove_noise).pipe(weight_filter).pipe(db_hit_layer_filter)\n",
    "    \n",
    "    #calculation filter \n",
    "    dfs = dfs.pipe(initial_params).pipe(p_t_filter,pt_cut).pipe(ita_filter,ita_min, ita_max)\n",
    "   \n",
    "   #less than 4 hit filter\n",
    "    dfs = nhit_filter(dfs, nhits)\n",
    "  \n",
    "    #order layers again\n",
    "    dfs = dfs.pipe(ordered_layer_id)\n",
    "\n",
    "    # construct graph segments (edges) in the correct range\n",
    "   \n",
    "    dfs = dfs.pipe(get_param_distr).pipe(edge_filter)\n",
    "\n",
    "    # save in different stages for analyisis \n",
    "    if SAVE:\n",
    "      newpath = f'/PATH_TO_SAVE1/'+f'{path_file}/Section_{sec}/'\n",
    "      if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "      dfs.to_csv(newpath+f'event{file_idx}.csv',index=False)\n",
    "    \n",
    "    #resample data \n",
    "    if BALANCE:\n",
    "      #only balance fro train set\n",
    "      dfs = dfs.pipe(resample_data).pipe(New_hit_id)\n",
    "    else:\n",
    "      dfs = dfs.pipe(New_hit_id)\n",
    "\n",
    "    if SAVE:\n",
    "      newpath = f'/PATH_TO_SAVE2/'+f'{path_file}/Section_{sec}/'\n",
    "      if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "      dfs.to_csv(newpath+f'event{file_idx}.csv',index=False)\n",
    "    \n",
    "    #data to graph \n",
    "    newpath = f'/PATH_TO_FINAL_GRAPH_DATA/'+f'{path_file}/Section_{sec}/'\n",
    "    if not os.path.exists(newpath):\n",
    "      os.makedirs(newpath)\n",
    "    data = data_to_graph(df)\n",
    "    #save grpah files \n",
    "    torch.save(data,newpath + f'data_{file_idx}.pt')\n",
    "    \n",
    "    \n",
    "  file_idx +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TNkKu3okFEsD"
   },
   "outputs": [],
   "source": [
    "#train\n",
    "run_and_save(0,60, 'Train', SAVE=True,BALANCE=True)\n",
    "#test\n",
    "run_and_save(60,90, 'Test', SAVE=True,BALANCE=False)\n",
    "#val\n",
    "run_and_save(90,100, 'Val', SAVE=True,BALANCE=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ss3n7mfoZkdB"
   },
   "outputs": [],
   "source": [
    "#master model\n",
    "def create_master_files(start,end,file_name):\n",
    "  indir = f'/PATH_TO/GRAPH_BALANCED/'\n",
    "  sections = np.arange(0,8)\n",
    "  files = []\n",
    "  for sec in sections:\n",
    "    print(sec)\n",
    "    sec_file = f'Section_{sec}/'\n",
    "    graph_files = np.array(os.listdir(indir + sec_file))\n",
    "    graph_files = [os.path.join(indir+sec_file,file) for file in graph_files]\n",
    "    files.append(graph_files)\n",
    "    newpath = f'/PATH_TO_MASTER/GRAPH_FINAL_{file_name}_MASTER/'\n",
    "    if not os.path.exists(newpath):\n",
    "      os.makedirs(newpath)\n",
    "  file_idx=0\n",
    "  for i,f in enumerate(files[0][start:end]):\n",
    "    for j in range(8):\n",
    "      \n",
    "      \n",
    "      df = pd.read_csv(files[j][i]) \n",
    "      data = data_to_graph(df)\n",
    "      torch.save(data,newpath + f'data_{file_idx}.pt')\n",
    "      file_idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Assn7fQRajrj"
   },
   "outputs": [],
   "source": [
    "create_master_files(0,60,'TEST')\n",
    "create_master_files(60,90,'TRAIN')\n",
    "create_master_files(90,100,'VAL')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Data_to_graph.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
