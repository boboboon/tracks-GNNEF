{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "DbRdnWSd3hxb"
   },
   "outputs": [],
   "source": [
    "#pip install --user git+https://github.com/LAL/trackml-library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "iG5KxZDA4GIe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"def format_pytorch_version(version):\\n    return version.split('+')[0]\\n\\nTORCH_version = torch.__version__\\nTORCH = format_pytorch_version(TORCH_version)\\n\\n#def format_cuda_version(version):\\n#    return 'cu' + version.replace('.', '')\\n\\n#CUDA_version = torch.version.cuda\\n#CUDA = format_cuda_version(CUDA_version)\\n\\n!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\\n!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\\n!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\\n!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\\n!pip install torch-geometric \""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "\"\"\"def format_pytorch_version(version):\n",
    "    return version.split('+')[0]\n",
    "\n",
    "TORCH_version = torch.__version__\n",
    "TORCH = format_pytorch_version(TORCH_version)\n",
    "\n",
    "#def format_cuda_version(version):\n",
    "#    return 'cu' + version.replace('.', '')\n",
    "\n",
    "#CUDA_version = torch.version.cuda\n",
    "#CUDA = format_cuda_version(CUDA_version)\n",
    "\n",
    "!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
    "!pip install torch-geometric \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "OQ6oxDZI4OH9"
   },
   "outputs": [],
   "source": [
    "#basic libraries\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import trackml.dataset\n",
    "import sys    \n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "#proccesing data \n",
    "from data_preprocess import *    # reads data from data_preprocess.py\n",
    "#ML libraries\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import namedtuple\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bvNwmHcm7xrH"
   },
   "source": [
    "## Data Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "98vs5Wpm5SpI"
   },
   "outputs": [],
   "source": [
    "#######################ENRICH DATA#####################################\n",
    "# Calculate derived hits variables \n",
    "def initial_params(df):\n",
    "    \"\"\" Calculate momentum radius, phi and ita\"\"\"\n",
    "    df['pt'] = np.sqrt(df.tpx**2 + df.tpy**2)\n",
    "    df['r'] = np.sqrt(df.x**2 + df.y**2)\n",
    "    df['phi'] = np.arctan2(df.y, df.x)\n",
    "    def calc_ita(r, z):\n",
    "        theta = np.arctan2(r, z)\n",
    "        return -1. * np.log(np.tan(theta / 2.)), theta\n",
    "    df['ita'], df['theta'] = calc_ita(df['r'],df['z'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def ordered_layer_id(df):\n",
    "    \"\"\"order layers from 0 to N\"\"\"\n",
    "    df = df.sort_values(by=['volume_id','layer_id','z']).reset_index(drop = True) \n",
    "    new_layers_array = np.zeros(len(df))\n",
    "    i = 0\n",
    "    idx = 0\n",
    "    for vol in df.volume_id.unique():\n",
    "        temp_df = df.loc[df.volume_id == vol]\n",
    "        for layer in temp_df.layer_id.unique():\n",
    "            temp2_df = temp_df.loc[temp_df.layer_id == layer]\n",
    "            new_layers_array[idx:len(temp2_df)+idx]=i\n",
    "            i+=1\n",
    "            idx+=len(temp2_df)\n",
    "    df['new_layer_id'] = new_layers_array.astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "V3O8x87u5Ewe"
   },
   "outputs": [],
   "source": [
    "#######################SECTION FILTERS#####################################\n",
    "# firts use the geomerty of the detector to reduce data, barrel, layers, and sectioning\n",
    "def sections(df,phi_sec,ita_sec, overlap_phi = 0, overlap_ita = 0):\n",
    "    \n",
    "    #use only important columns\n",
    "    df = df[['new_hit_id', 'new_particle_id', 'particle_id','weight','new_layer_id','layer_id','volume_id','x', 'y', 'z','pt','tpx','tpy', 'r', 'phi','ita','event_id']]\n",
    "    phi_range = (-np.pi, np.pi)\n",
    "    ita_range =  (-3, 3)\n",
    "    n_phi_sections = phi_sec          # Towards a realistic track reconstruction algorithm based\n",
    "    n_ita_sections = ita_sec          #on graph neural networks for the HL-LHC\n",
    "    \n",
    "    phi_edges = np.linspace(*phi_range, num=n_phi_sections+1)   #4 sections in range of phi \n",
    "    ita_edges = np.linspace(*ita_range, num=n_ita_sections+1)   #2 sections in range of ita\n",
    "    def split_detector_sections(df, phi_edges, ita_edges):\n",
    "        \"\"\"Split hits according to provided phi and ita boundaries.\"\"\"\n",
    "        hits_sections = []\n",
    "        # Loop over sections on phi\n",
    "        for i in range(len(phi_edges) - 1):\n",
    "            if np.round(abs(phi_edges[i] - overlap_phi),4)>= 3.1416:     # when angle - overlap is smaller than -pi make angle positive and join dfs in range \n",
    "                \n",
    "                phi_max = phi_edges[i+1] + overlap_phi\n",
    "                phi_min = ((phi_edges[i] - overlap_phi) + 2*np.pi) \n",
    "                pos = df.loc[df.phi > phi_min]\n",
    "                neg = df.loc[df.phi <= phi_max]\n",
    "                phi_hits = pd.concat((pos,neg))\n",
    "                \n",
    "            elif np.round(abs(phi_edges[i+1] + overlap_phi),4)>= 3.1416:  # when angle + overlap is bigger than pi make angle negative and join dfs in range \n",
    "                \n",
    "                phi_min = phi_edges[i] - overlap_phi\n",
    "                phi_max = ((phi_edges[i+1] + overlap_phi) - 2*np.pi) \n",
    "                pos = df.loc[df.phi > phi_min]\n",
    "                neg = df.loc[df.phi <= phi_max]\n",
    "                phi_hits = pd.concat((pos,neg))\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                phi_min = phi_edges[i] - overlap_phi\n",
    "                phi_max = phi_edges[i+1] + overlap_phi\n",
    "                phi_hits = df[(df.phi >= phi_min) & (df.phi < phi_max)]\n",
    "                \n",
    "                \n",
    "            # Center these hits on phi=0\n",
    "            centered_phi = phi_hits.phi - (phi_min + phi_max) / 2\n",
    "            phi_hits = phi_hits.assign(sec_phi=centered_phi, phi_section=i)\n",
    "            #loop over sections on phi\n",
    "            for j in range(len(ita_edges) - 1):\n",
    "                ita_min, ita_max = ita_edges[j] - overlap_ita, ita_edges[j+1] + overlap_phi\n",
    "                # Select hits in this ita section\n",
    "                sec_hits = phi_hits[(df['ita'] >= ita_min) & (df['ita'] < ita_max)]\n",
    "                hits_sections.append(sec_hits.assign(ita_section=j))\n",
    "            #create new hits ids \n",
    "        for i ,dfs in enumerate(hits_sections):\n",
    "            dfs = dfs.reset_index(drop = True)\n",
    "            dfs.new_hit_id = dfs.index\n",
    "            hits_sections[i] = dfs\n",
    "            \n",
    "        return hits_sections\n",
    "    return  split_detector_sections(df, phi_edges, ita_edges)\n",
    "\n",
    "\n",
    "#barrel\n",
    "def volume_filter(df, volume_list):\n",
    "    \"\"\" \n",
    "     Returns only hits in volumes given in volume_list \n",
    "    \"\"\"\n",
    "    return df.loc[df.volume_id.isin(volume_list)]\n",
    "\n",
    "#choosen layers\n",
    "def layer_filter(df, layer_list):\n",
    "    \"\"\" \n",
    "     Returns only hits in volumes given in volume_list \n",
    "    \"\"\"\n",
    "    return df.loc[df.new_layer_id.isin(layer_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "exjfaEUu4yxm"
   },
   "outputs": [],
   "source": [
    "#######################INITIAL FILTERS#####################################\n",
    "def remove_noise(df):\n",
    "    return df.loc[df.particle_id!=0]\n",
    "def weight_filter(df):\n",
    "    \"\"\"\n",
    "     Remove weights = 0\n",
    "    \"\"\"\n",
    "    return df.loc[df.weight!=0]\n",
    "\n",
    "def db_hit_layer_filter(df):\n",
    "    \"\"\"\n",
    "     Remove double hits in the same layer\n",
    "    \"\"\"\n",
    "    return df.loc[df.duplicated(subset=['new_particle_id', 'layer_id','volume_id'])==False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "xi38nVLU57gy"
   },
   "outputs": [],
   "source": [
    "#######################CALCULATION FILTERS#####################################\n",
    "def p_t_filter(df, min_pt):\n",
    "    \"\"\"Removes particles < min_pt\"\"\"\n",
    "    pt_1 = df.loc[df['pt']>min_pt].new_particle_id.unique()\n",
    "    return df.loc[df['new_particle_id'].isin(pt_1)]\n",
    "def ita_filter(df, min_ita, max_ita):\n",
    "    \"\"\"keeps particles in the range (min_ita,max_ita)\"\"\"\n",
    "    ita_f = df.loc[(df['ita']<min_ita) | (df['ita']>max_ita)].new_particle_id.unique()\n",
    "    return df.loc[~df['new_particle_id'].isin(ita_f)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4hVp-5y75cy"
   },
   "source": [
    "## Node Edge Conections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "5zewC9_v6VYS"
   },
   "outputs": [],
   "source": [
    "#######################MORE THAN 4 FILTERS#####################################\n",
    "def nhit_filter(df, nhits):\n",
    "    \"\"\"\n",
    "        Remove less than n hits\n",
    "    \"\"\"\n",
    "    #find the amounts of hits per particle\n",
    "    new_nhits = df.new_particle_id.value_counts().rename_axis('new_particle_id').reset_index(name='new_nhits') \n",
    "    #merge the new hhits on new particle id \n",
    "    df = df.merge(new_nhits, on='new_particle_id')\n",
    "    return df.loc[df.new_nhits >= nhits]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "FnVT3iRo8Mbx"
   },
   "outputs": [],
   "source": [
    "#######################CREATE EDGES#####################################\n",
    "def calc_dphi(phi1, phi2):\n",
    "    \"\"\"Computes phi2-phi1 \"\"\"\n",
    "    dphi = phi2 - phi1\n",
    "    return dphi\n",
    "\n",
    "def get_param_distr(df):\n",
    "    \"\"\" create edges and calculates dphi, dz,dr,dita,dR,phi_slope, and z0\"\"\"\n",
    "    \n",
    "    dR_list, phi_slope_list,z0_list, hits_pair_list = [], [], [],[]\n",
    "    #################CREATE EDGES#####################################\n",
    "    # some particle ids dont have hist in certain layers therefore is not possible to just loop through conecutive layers, \n",
    "    # it is neccesary to loop through each p_id to see the set of consecutive layers and conect those layers\n",
    "    \n",
    "    for p_id in df.new_particle_id.unique():\n",
    "        \n",
    "        temp_df = df.loc[df.new_particle_id==p_id]\n",
    "        cons_layers = temp_df.new_layer_id.unique()\n",
    "    # loop trhough layers to generate hits pair(section of track) on consecutive layers \n",
    "        for i in range(len(cons_layers)-1):\n",
    "            \n",
    "            hits1 = temp_df.loc[temp_df.new_layer_id == cons_layers[i]]\n",
    "            hits2 = df.loc[df.new_layer_id == cons_layers[i+1]]\n",
    "            keys = ['event_id', 'new_hit_id','new_layer_id','new_particle_id','r', 'phi', 'z','ita','x','y']\n",
    "            hit_pairs  = hits1[keys].reset_index(drop = True).merge(hits2[keys].reset_index(drop = True), on='event_id', suffixes=('_1', '_2'))\n",
    "            \n",
    "            hit_pairs['y'] = hit_pairs.new_particle_id_1 == hit_pairs.new_particle_id_2\n",
    "            \n",
    "            hits_pair_list.append(hit_pairs)\n",
    "            \n",
    "        # concatenate firts 2 hit pairs \n",
    "    df_concat = pd.concat([hits_pair_list[0], hits_pair_list[1]], ignore_index=True)\n",
    "    # concatenate firts the rest hit pairs\n",
    "    for i in range(len(hits_pair_list) -2):\n",
    "        df_concat = pd.concat([df_concat, hits_pair_list[i+2]], ignore_index=True)\n",
    "        \n",
    "    #############CALCULATE PARAMTERS#####################################  \n",
    "    df_concat['dphi'] = calc_dphi(df_concat.phi_1, df_concat.phi_2)\n",
    "    df_concat['dz'] = df_concat.z_2 - df_concat.z_1\n",
    "    df_concat['dr'] = df_concat.r_2 - df_concat.r_1\n",
    "    df_concat['dita'] = df_concat.ita_2 - df_concat.ita_1\n",
    "    df_concat['dR'] = np.sqrt(df_concat['dita']**2 + df_concat['dphi']**2)\n",
    "    df_concat['phi_slope'] = df_concat['dphi'] / df_concat['dr']\n",
    "    df_concat['z0'] = df_concat.z_1 - df_concat.r_1 * df_concat['dz'] / df_concat['dr']\n",
    "    return df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "uPtLxWtE9L_Y"
   },
   "outputs": [],
   "source": [
    "def edge_filter(df):\n",
    "    \"\"\"reduce amount of edges using calculated parameters\"\"\"\n",
    "    min_z0 =  -320\n",
    "    max_z0 = 520\n",
    "    df = df.loc[(df.z0>=min_z0) & (df.z0<=max_z0)]\n",
    "    max_dr = 0.5\n",
    "    df = df.loc[df.dR<=max_dr]\n",
    "    \n",
    "    min_phi_slope =  -.0055\n",
    "    max_phi_slope = .0055\n",
    "    df =df.loc[(df.phi_slope>=min_phi_slope) & (df.phi_slope<max_phi_slope)] \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1F-GLjv49inb"
   },
   "source": [
    "## Inbalanced Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "0hQIhKCV9wDP"
   },
   "outputs": [],
   "source": [
    "#############BALANCE DATASET##################################### \n",
    "def resample_data(df):\n",
    "    \"\"\"Returns balanced dataset\"\"\"\n",
    "    X = df.drop(['y'], axis=1)\n",
    "    y = df.y\n",
    "    \n",
    "    #undersample data \n",
    "    rus = RandomUnderSampler(random_state=42)\n",
    "    X_res, y_res = rus.fit_resample(X, y)\n",
    "    \n",
    "    df = X_res\n",
    "    df['y'] = y_res\n",
    "    return df.sort_values(by=['new_layer_id_1','z_1'])\n",
    "\n",
    "##########RE-LABLE HIT ID#####################################\n",
    "def New_hit_id(df):\n",
    "    \"\"\"returns dataset with ordered hit id\"\"\"\n",
    "    df = df.sort_values(by=['new_layer_id_1','z_1'])\n",
    "    df_hits = pd.DataFrame()\n",
    "    df_hits['unique_hits'] =  pd.concat([df['new_hit_id_1'], df['new_hit_id_2']]).unique()\n",
    "    df_hits = df_hits.sort_values('unique_hits').reset_index(drop = True)\n",
    "    df_hits['new_hit'] = df_hits.index\n",
    "    \n",
    "    newest_id1 = np.zeros(len(df))\n",
    "    \n",
    "    for i, id in enumerate(df['new_hit_id_1']):\n",
    "        newest_id1[i] = df_hits.new_hit.loc[df_hits.unique_hits==id]\n",
    "    df['hit_id_1'] = newest_id1.astype(int)\n",
    "    newest_id2 = np.zeros(len(df))\n",
    "    \n",
    "    for i, id in enumerate(df['new_hit_id_2']):\n",
    "        newest_id2[i] = df_hits.new_hit.loc[df_hits.unique_hits==id]\n",
    "    df['hit_id_2'] = newest_id2.astype(int)\n",
    "    df = df.sort_values('hit_id_1')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wFwVjzfP-8Z8"
   },
   "source": [
    "## Data in Graph Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "d1_jhXSQ_GzZ"
   },
   "outputs": [],
   "source": [
    "def data_to_graph(df,directed=True):\n",
    "    \"\"\"Returns graph data into pytorch format\"\"\"\n",
    "    edge_attr = df[['dphi', 'dz', 'dr', 'dita', 'dR','phi_slope', 'z0']]\n",
    "    edge_index = df[['hit_id_1','hit_id_2']]\n",
    "    y = df.y\n",
    "    df1 = pd.DataFrame()\n",
    "    df2 = pd.DataFrame()\n",
    "    df1[['hit_id','r', 'phi', 'z', 'ita', 'x','y' ]] = df[['hit_id_1','r_1', 'phi_1', 'z_1', 'ita_1','x_1','y_1' ]]\n",
    "    df2[['hit_id','r', 'phi', 'z', 'ita', 'x','y' ]] = df[['hit_id_2','r_2', 'phi_2', 'z_2', 'ita_2','x_2','y_2' ]]\n",
    "    x = pd.concat([df1,df2])\n",
    "    x = x.drop_duplicates(subset=['hit_id'])\n",
    "    # x = x.sort_values('hit_id')\n",
    "    x = x[['r', 'phi', 'z', 'ita', 'x','y']]\n",
    "    \n",
    "    if directed:\n",
    "        edge_attr = torch.from_numpy(edge_attr.to_numpy()).type(torch.FloatTensor)\n",
    "        edge_index = torch.from_numpy(edge_index.to_numpy())   # this is double \n",
    "        y = torch.from_numpy(y.to_numpy()).type(torch.FloatTensor)\n",
    "        x = torch.from_numpy(x.to_numpy()).type(torch.FloatTensor)    #change to float  x[['r', 'phi', 'z', 'ita', 'x','y']]\n",
    "        data = Data(x=x, edge_index=edge_index.T,edge_attr=edge_attr,y=y)\n",
    "    else:\n",
    "        \n",
    "        edge_index_und = torch.stack([torch.cat([edge_index[:,0], edge_index[:,1]], dim=0), torch.cat([edge_index[:,1], edge_index[:,0]], dim=0)], dim = 0)\n",
    "        edge_attr_und = torch.cat([edge_attr, -1*edge_attr], dim=0)\n",
    "        y_und = torch.cat([y,y])\n",
    "        data = Data(x=x,edge_index= edge_index_und,edge_attr = edge_attr_und,y = y_und)#, first_layer_hits = first_layer_hits  )\n",
    "    return data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z61QVeG-_hQ1"
   },
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "eSDiMOMK_mXz"
   },
   "outputs": [],
   "source": [
    "from time import time as tt\n",
    "\n",
    "##########CONSTANTS#####################################\n",
    "volume_list = [8,13,17]   #barrel\n",
    "pt_cut = 1\n",
    "ita_min = -5\n",
    "ita_max = 5\n",
    "nhits = 5\n",
    "layer_list = [0,1,2,3,4,5,7,8]\n",
    "\n",
    "def run_and_save(event_start,event_end,input_dir, output_dir, path_file):\n",
    "    \"\"\"transforms raw data into graph format as outlined in the report and save it\"\"\"\n",
    "    for file_idx, i in enumerate(tqdm(range(event_start,event_end))):\n",
    "        #read data \n",
    "        tic = tt()\n",
    "        hits, particles, truth = read_all_data(input_dir, 1000 + i,1000 + (i+1))\n",
    "        df = merge_dfs(hits, particles, truth)\n",
    "\n",
    "        #firts volume filter\n",
    "        df = volume_filter(df, volume_list)\n",
    "\n",
    "        #fits stage create new_layer_id \n",
    "        df = ordered_layer_id(df)\n",
    "\n",
    "        #then return only layers in list \n",
    "        df = layer_filter(df, layer_list)\n",
    "\n",
    "        #calculate the extra parameters\n",
    "        df = initial_params(df)\n",
    "\n",
    "        #section detector\n",
    "        df_sections = sections(df,4,2, overlap_phi = 0.26, overlap_ita = 0.5)\n",
    "\n",
    "        #loop throug all sections \n",
    "        for sec,dfs in enumerate(df_sections):\n",
    "            #initial filters\n",
    "            dfs = dfs.pipe(remove_noise).pipe(weight_filter).pipe(db_hit_layer_filter)\n",
    "\n",
    "            #calculation filter \n",
    "            dfs = dfs.pipe(initial_params).pipe(p_t_filter,pt_cut).pipe(ita_filter,ita_min, ita_max)\n",
    "\n",
    "            #less than 4 hit filter\n",
    "            dfs = nhit_filter(dfs, nhits)\n",
    "\n",
    "            #order layers again\n",
    "            dfs = dfs.pipe(ordered_layer_id)\n",
    "\n",
    "            # construct graph segments (edges) in the correct range\n",
    "            dfs = dfs.pipe(get_param_distr)\n",
    "\n",
    "            dfs = dfs.pipe(edge_filter)\n",
    "\n",
    "            dfs = dfs.pipe(New_hit_id)\n",
    "\n",
    "            #create graph data\n",
    "            data = data_to_graph(dfs)\n",
    "\n",
    "            #data to graph \n",
    "            newpath = os.path.join(output_dir, path_file, f\"Section_{sec}\")\n",
    "            if not os.path.exists(newpath):\n",
    "                os.makedirs(newpath)\n",
    "            #save grpah files \n",
    "            torch.save(data, os.path.join(newpath, f\"graph_{file_idx}.pt\"))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [10:34<00:00, 63.46s/it]\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "input_dir = \"/Users/lucascurtin/Desktop/QGNN Repos/tracks-GNNEF/trackML/sample_data/train_100_events\"\n",
    "output_dir = \"/Users/lucascurtin/Desktop/QGNN Repos/tracks-GNNEF/Output_data\"\n",
    "#run_and_save(0,80, input_dir, output_dir, 'Train')\n",
    "run_and_save(80,90, input_dir, output_dir, 'Test')\n",
    "#run_and_save(90,100, input_dir, output_dir, 'Val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Data_to_graph.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "e8d8d42a8f729872fc17dc6c613b8965788883a5ed2f18a08b24ecb8c800a303"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
