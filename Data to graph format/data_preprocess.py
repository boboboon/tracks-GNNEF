# -*- coding: utf-8 -*-
"""Data_preprocess.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14zcUgnN3Cd3Lr7mUwhH76NtN7ryLjyND
"""



import numpy as np 
import pandas as pd
import trackml.dataset
import time

#files from 1000-1099
def read_all_data(start, end):
    t0 = time.time()
    hits_list = []
    particle_list = []
    truth_list = []
    event_id = []   #to identify repeated particle ids 
    #how many files do we want?
    file_range = np.arange(start,end)
    #iteraletvely read the data and save 
    for file in file_range:
        hits, particles, truth = trackml.dataset.load_event(f'../train_100_events/event00000{file}', parts=['hits', 'particles', 'truth'])
        event_id = [file] * len(hits)
        hits['event_id'] = event_id
        hits_list.append(hits)
        event_id = [file] * len(particles)
        particles['event_id'] = event_id
        particles['new_particle_id'] = particles.particle_id + particles.event_id  #create unique particle ids 
        particle_list.append(particles)
        truth_list.append(truth)
    
    if len(file_range) == 1:
        #print("... read data completed in {0} seconds".format(time.time()-t0))
        return hits, particles, truth

    else:
        #concateante dfs
        hits = pd.concat([hits_list[0], hits_list[1]], ignore_index=True)
        particles = pd.concat([particle_list[0], particle_list[1]], ignore_index=True)
        truth = pd.concat([truth_list[0], truth_list[1]], ignore_index=True)
        if len(hits_list) == 2:
            #print("... read data completed in {0} seconds".format(time.time()-t0))
            return hits, particles, truth
    
        else:
            for i in range(len(hits_list)-2):
                hits = pd.concat([hits, hits_list[i+2]], ignore_index=True)
                particles = pd.concat([particles, particle_list[i+2]], ignore_index=True)
                truth = pd.concat([truth, truth_list[i+2]], ignore_index=True)
            #print("... read data completed in {0} seconds".format(time.time()-t0))
            return hits, particles, truth


#remove paricle id = 0, create new hits ids, remove zero particle id 
def merge_dfs(hits, particles, truth):
    """
     Returns a single df with all the desired information from n dfs
    """

    #time it 
    t0 = time.time()
    hits['new_hit_id'] = hits.index
    truth['new_hit_id'] = truth.index
    df = truth.merge(hits, on='new_hit_id', suffixes=('_H', '_T'))
    # df = df.loc[df.particle_id!=0]
    df['new_particle_id'] = df.particle_id + df.event_id
    # df = df.merge(particles[['new_particle_id','vx', 'vy', 'vz', 'px', 'py', 'pz', 'q', 'nhits',]], on='new_particle_id')
    #print("... merge df completed in {0} seconds".format(time.time()-t0))
    return df 


