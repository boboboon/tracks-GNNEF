# -*- coding: utf-8 -*-
"""GNN_train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Db9zwHZy1dKSBm9zhXOBUFWvzYXaG7hz
"""

#basic
import numpy as np 
import pandas as pd
import time
import os

#NN
import torch
def format_pytorch_version(version):
  return version.split('+')[0]

TORCH_version = torch.__version__
TORCH = format_pytorch_version(TORCH_version)

def format_cuda_version(version):
  return 'cu' + version.replace('.', '')

CUDA_version = torch.version.cuda
CUDA = format_cuda_version(CUDA_version)

!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html
!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html
!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html
!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html
!pip install torch-geometric 
from torch_geometric.data import Data, Dataset#,DataLoader
from torch_geometric.loader import DataLoader
from torch import Tensor
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torch_geometric.transforms as Tr
from torch_geometric.nn import MessagePassing
from torch_geometric.utils import to_networkx
from torch.nn import Sequential as Seq, Linear, ReLU, Sigmoid
from torch.optim.lr_scheduler import StepLR
from collections import namedtuple

import warnings
warnings.filterwarnings("ignore")

"""## Dataset and Dataloader"""

class GraphDataset(Dataset):
    def __init__(self,graph_files,file_name,transform=None, pre_transform=None):
        super(GraphDataset,self).__init__()

        self.graph_files = graph_files
        self.file_name = file_name
    
    @property                 
    def raw_file_names(self):
      return self.graph_files

    @property
    def processed_file_names(self):
      return []

   
        
    def get(self, idx):
      data = torch.load(f'/PATH_TO_FINAL_GRAPH_DATA/{self.file_name }' + f'data_{idx}.pt')
      return data    
          
    def len(self):
      return len(self.graph_files)

######################READ FILES##############
home_dir = "../"
test_f ='TEST_MASTER/'
train_f = 'TRAIN_MASTER/'
val_f = 'VAL_MASTER/'
indir = '/content/drive/MyDrive/Colab Notebooks/'
    
graph_files_test = np.array(os.listdir(indir + test_f))
graph_files_test = [os.path.join(indir + test_f,file)
                           for file in graph_files_test]
graph_files_train = np.array(os.listdir(indir+train_f))
graph_files_train = [os.path.join(indir+train_f, file)
                            for file in graph_files_train]
graph_files_val = np.array(os.listdir(indir+val_f))

#####################GET THE DATALOADER FOR EACH SET##############
params = {'batch_size': 16, 'shuffle': True, 'num_workers': 2}
train_set = GraphDataset(graph_files_train,train_f)
train_loader = DataLoader(train_set,**params)  #batches join graphs instead of splitting them therefore more than train set batches will make 1 batch only 
test_set = GraphDataset(graph_files_test, test_f)
test_loader = DataLoader(test_set, **params)
val_set = GraphDataset(graph_files_val, val_f)
val_loader = DataLoader(val_set, **params)

"""## NN Architecture"""

class RelationalModel(nn.Module):
    def __init__(self, input_size, output_size, hidden_size):
        super(RelationalModel, self).__init__()
        
        self.layers = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, output_size),
        )

    def forward(self, m):
        return self.layers(m)

class ObjectModel(nn.Module):
    def __init__(self, input_size, output_size, hidden_size):
        super(ObjectModel, self).__init__()
        
        self.layers = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, output_size),
        )

    def forward(self, C):
        return self.layers(C)


class InteractionNetwork(MessagePassing):
    def __init__(self, node_f_size, edge_attr_size,message_out, update_out, hidden_size):
        super(InteractionNetwork, self).__init__(aggr='add', 
                                                 flow='source_to_target')
        self.R1 = RelationalModel(2*node_f_size + edge_attr_size, message_out, hidden_size)    # 19 is the node_features * 2 + edge atributes output 4 
        self.O = ObjectModel(node_f_size + message_out, update_out, hidden_size)    # 10 is node features + output R1
        self.R2 = RelationalModel(2*update_out + message_out , 1, hidden_size)  #10 is from 2* output O + output R1(from the concat) 
        self.E: Tensor = Tensor()

    def forward(self, x: Tensor, edge_index: Tensor, edge_attr: Tensor) -> Tensor:
        x_tilde = self.propagate(edge_index, x=x, edge_attr=edge_attr, size=None)
        m2 = torch.cat([x_tilde[edge_index[1]],
                        x_tilde[edge_index[0]],
                        self.E], dim=1)
        return torch.sigmoid(self.R2(m2))
        
    def message(self, x_i, x_j, edge_attr):
        m1 = torch.cat([x_i, x_j, edge_attr], dim=1)
        self.E = self.R1(m1)
        return self.E

    def update(self, aggr_out, x):
        c = torch.cat([x, aggr_out], dim=1)
        return self.O(c)

#initialize device
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)

###############################TUNABLE PARAMETERS###############
#define the size of layers on the nn 
hidden_l_size = 16   #tunable parameter
message_out = 4       #tunable parameter
update_out = 3        #tunable parameter
edge_attr_size = train_set.get(0).edge_attr.shape[1]
node_f_size = train_set.get(0).x.shape[1]
#initialize model 
model = InteractionNetwork(node_f_size = node_f_size, edge_attr_size = edge_attr_size,message_out = message_out, update_out= update_out,hidden_size = hidden_l_size).to(device)
#MORE TUNABLE PARAMETERS
optimizer = optim.Adam(model.parameters(), lr=0.005)
scheduler = StepLR(optimizer, step_size=10,
                       gamma=0.9)               #https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html

def binary_acc(y_pred, y_test,thld):
  """
  returns accuracy based on a given treshold
  """

  # true positives edges with ouput prediction bigger than thld(1) and label = 1
  TP = torch.sum((y_test==1.).squeeze() & 
                           (y_pred>thld).squeeze()).item()
  # true negatives edges with ouput prediction smaller than thld(0) and label = 0
  TN = torch.sum((y_test==0.).squeeze() & 
                           (y_pred<thld).squeeze()).item()
  # False positives edges with ouput prediction bigger than thld(1) and label = 0
  FP = torch.sum((y_test==0.).squeeze() & 
                           (y_pred>thld).squeeze()).item()
  # False negatives edges with ouput prediction smaller than thld(0) and label = 1                     
  FN = torch.sum((y_test==1.).squeeze() & 
                           (y_pred<thld).squeeze()).item() 
  #how many correct predictions are made, if FP = 0 and FN = 0 acc = 1                       
  acc = (TP+TN)/(TP+TN+FP+FN)
    
  return acc

"""## Training model"""

def binary_acc(y_pred, y_test,thld):
  """
  returns accuracy based on a given treshold
  """

  # true positives edges with ouput prediction bigger than thld(1) and label = 1
  TP = torch.sum((y_test==1.).squeeze() & 
                           (y_pred>thld).squeeze()).item()
  # true negatives edges with ouput prediction smaller than thld(0) and label = 0
  TN = torch.sum((y_test==0.).squeeze() & 
                           (y_pred<thld).squeeze()).item()
  # False positives edges with ouput prediction bigger than thld(1) and label = 0
  FP = torch.sum((y_test==0.).squeeze() & 
                           (y_pred>thld).squeeze()).item()
  # False negatives edges with ouput prediction smaller than thld(0) and label = 1                     
  FN = torch.sum((y_test==1.).squeeze() & 
                           (y_pred<thld).squeeze()).item() 
  #how many correct predictions are made, if FP = 0 and FN = 0 acc = 1                       
  acc = (TP+TN)/(TP+TN+FP+FN)
    
  return acc

preccision_acc = 0.5
def train( model, device, train_loader, optimizer, epoch):
    """train model"""
    model.train()
    times_list = []
    losses = []
    accs = []
    epoch_t0 = time()
    for batch_idx, data in enumerate(train_loader):
        data = data.to(device)
        optimizer.zero_grad()
        output = model(data.x, data.edge_index, data.edge_attr)
        y, output = data.y, output.squeeze(1)
        loss = F.binary_cross_entropy(output, y, reduction='mean')
        loss.backward()
        optimizer.step()
      
        losses.append(loss.item())   
        accs.append(binary_acc(y_pred = output, y_test = y, thld = preccision_acc ))
    times = time()-epoch_t0
    times_list.append(times)
    print(f"...epoch time: {times}s")
    print(f"...epoch {epoch}: mean train loss={np.mean(losses):.6f}......train acc={np.mean(accs):.6f}")
    return losses, accs,times_list

def validate(model, device, val_loader):
    """validates model, returns optimal treshold"""
    model.eval()
    opt_thlds, accs = [], []
    for batch_idx, data in enumerate(val_loader):
        data = data.to(device)
        output = model(data.x, data.edge_index, data.edge_attr)
        y, output = data.y, output.squeeze()
        loss = F.binary_cross_entropy(output, y, reduction='mean').item()
        
        # define optimal threshold (thld) where TPR = TNR 
        diff, opt_thld, opt_acc = 100, 0, 0
        best_tpr, best_tnr = 0, 0
        for thld in np.arange(0.001, 0.5, 0.001):
            TP = torch.sum((y==1) & (output>thld)).item()
            TN = torch.sum((y==0) & (output<thld)).item()
            FP = torch.sum((y==0) & (output>thld)).item()
            FN = torch.sum((y==1) & (output<thld)).item()
            acc = (TP+TN)/(TP+TN+FP+FN)
            TPR, TNR = TP/(TP+FN), TN/(TN+FP)
            delta = abs(TPR-TNR)
            if (delta < diff): 
                diff, opt_thld, opt_acc = delta, thld, acc

        opt_thlds.append(opt_thld)
        accs.append(opt_acc)

    print(f"...........................................val acc={np.mean(accs):.6f}")
    return np.mean(opt_thlds) 

def test(model, device, test_loader, thld=0.5):
    """test model"""
    model.eval()
    test_t0 = time()
    losses, accs = [], []
    with torch.no_grad():
        for batch_idx, data in enumerate(test_loader):
            data = data.to(device)
            output = model(data.x, data.edge_index, data.edge_attr)
            acc = binary_acc(y_pred = output, y_test = data.y, thld =  thld)
            loss = F.binary_cross_entropy(output.squeeze(1), data.y, 
                                          reduction='mean').item()
            accs.append(acc)
            losses.append(loss)
    
    #when batching works change acc for mean accs
    print(f"...testing time: {time()-test_t0}s")
    print(f'.............mean test loss={np.mean(losses):.6f}......test acc ={acc:.6f}\n')
    return np.mean(losses), np.mean(accs)

EPOCH = 200
model = InteractionNetwork(node_f_size = node_f_size, edge_attr_size = edge_attr_size,message_out = message_out, update_out= update_out,hidden_size = hidden_l_size).to(device)
optimizer = optim.Adam(model.parameters(), lr=0.005)
scheduler = StepLR(optimizer, step_size=10,
                       gamma=0.9)       
from time import time
output = {'train_loss': [], 'test_loss': [], 'test_acc': []}
losses_l, accs_l,times_l = [],[],[]
for epoch in range(1, EPOCH + 1):
  print("---- Epoch {} ----".format(epoch))
  losses, accs,times = train( model, device, train_loader, optimizer, epoch)
  losses_l.append(losses)
  accs_l.append(accs)
  times_l.append(times)
  thld = validate(model, device, val_loader)
  print('...optimal threshold', thld)
  test_loss, test_acc = test(model, device, test_loader, thld=thld)
  scheduler.step()

torch.save(model.state_dict(),  '/PATH_TO_MOLDES/GRAPH_MODELS/model_master.pt')